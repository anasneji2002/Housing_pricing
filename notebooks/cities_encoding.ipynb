{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cities/Regions Encoding**\n",
    "\n",
    "### **What to Expect ?**\n",
    "\n",
    "This notebook aims to do the following \n",
    " - Encode the cities\n",
    " - Encode the regions\n",
    "\n",
    "This will be done by creating 2 maps\n",
    " - one for cities encoding\n",
    " - one for regions encoding\n",
    "\n",
    "And replacing the string values by their encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## necessary imports here\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions/consts\n",
    "\n",
    "def getFilePath(fileName, isSaveMapContext):\n",
    "    templateToUse = \"C:\\\\Users\\\\mohamedanas.neji\\\\OneDrive - Medius\\\\Desktop\\\\Housing_pricing\\\\utils\\\\\" if isSaveMapContext else \"C:\\\\Users\\\\mohamedanas.neji\\\\OneDrive - Medius\\\\Desktop\\\\Housing_pricing\\\\data\\\\raw\\\\\" \n",
    "    return f\"{templateToUse}{fileName}\"\n",
    "\n",
    "## dataset_clean.csv has location and city but city is more precise so we will be removing location in a seperate notebook\n",
    "## tunisia-real-estate.csv has duplicated columns (Delegation and Locality) so we will be removing locality in another notebook\n",
    "## tunisie_annonce_data(1).csv doesnt have a column for cities so we won't do them\n",
    "FILES_TO_CITIES_COLUMN = {\n",
    "    \"dataset_clean.csv\": \"city\", \n",
    "    \"Property Prices in Tunisia.csv\": \"region\",\n",
    "    \"tayara.csv\": \"region\",\n",
    "    \"TechnocasaDataset.csv\": \"Subtitle\",\n",
    "    \"tunisia-real-estate.csv\": \"Delegation\"\n",
    "}\n",
    "\n",
    "## TechnocasaDataset.csv doesn't have a column for region so we won't be doing them\n",
    "FILES_TO_REGION_COLUMN = {\n",
    "    \"dataset_clean.csv\": \"governorate\", \n",
    "    \"Property Prices in Tunisia.csv\": \"city\",\n",
    "    \"tayara.csv\": \"city\",\n",
    "    \"tunisia-real-estate.csv\": \"Governorate\",\n",
    "    \"tunisie_annonce_data (1).csv\": \"Gouvernorat\"\n",
    "}\n",
    "\n",
    "FILES_TO_PRICE_COLUMN = {\n",
    "    \"dataset_clean.csv\": \"price_tnd\", \n",
    "    \"Property Prices in Tunisia.csv\": \"price\",\n",
    "    \"tayara.csv\": \"price\",\n",
    "    \"tunisia-real-estate.csv\": \"Price\",\n",
    "    \"tunisie_annonce_data (1).csv\": \"Price\",\n",
    "    \"TechnocasaDataset.csv\": \"Price\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. cities encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a- Create the cities encoding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 'mohamadia' with 'mohamedia' in 'dataset_clean.csv'.\n",
      "Replaced 'mégrine' with 'megrine' in 'dataset_clean.csv'.\n",
      "Replaced 'el omrane superieur' with 'omrane superieur' in 'dataset_clean.csv'.\n",
      "Replaced 'ouardia' with 'el ouardia' in 'Property Prices in Tunisia.csv'.\n",
      "Replaced 'mohamadia' with 'mohamedia' in 'tunisia-real-estate.csv'.\n",
      "Replaced 'mannouba' with 'manouba' in 'tunisia-real-estate.csv'.\n",
      "Replaced 'kabbaria' with 'kabaria' in 'tunisia-real-estate.csv'.\n",
      "Replaced 'ouerdia' with 'el ouardia' in 'tunisia-real-estate.csv'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m             city_prices_map[city] \u001b[38;5;241m=\u001b[39m [price]      \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# get the average of prices for each city\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m city_to_avg_prices \u001b[38;5;241m=\u001b[39m {city: np\u001b[38;5;241m.\u001b[39maverage(city_prices_map[city]) \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m city_prices_map\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(city_to_avg_prices)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Create a consistent mapping\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 37\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m             city_prices_map[city] \u001b[38;5;241m=\u001b[39m [price]      \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# get the average of prices for each city\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m city_to_avg_prices \u001b[38;5;241m=\u001b[39m {city: np\u001b[38;5;241m.\u001b[39maverage(city_prices_map[city]) \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m city_prices_map\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(city_to_avg_prices)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Create a consistent mapping\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "city_prices_map = {}\n",
    "value_changes_map = {\"mohamadia\": \"mohamedia\", \"mannouba\": \"manouba\", \"mégrine\": \"megrine\", \"kabbaria\": \"kabaria\", \"ouardia\": \"el ouardia\",\"ouerdia\": \"el ouardia\", \"el omrane superieur\": \"omrane superieur\"}\n",
    "for file, cityColumn in FILES_TO_CITIES_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "    priceColumn = FILES_TO_PRICE_COLUMN[file]\n",
    "    data = pd.read_csv(filePath)\n",
    "    has_changed = False\n",
    "    nan_count = data[cityColumn].isna().sum()\n",
    "    data_cleaned = data\n",
    "    if(nan_count > 0):\n",
    "        has_changed = True\n",
    "        # Remove rows with NaN in the city column\n",
    "        data_cleaned = data.dropna(subset=[cityColumn])\n",
    "        print(f\"File '{file}' has {len(data) - len(data_cleaned)} rows removed due to missing '{cityColumn}' values.\")\n",
    "    \n",
    "    # Some cities needs to be changed to ave the same name (sahitek sidahmed hhhhhh)\n",
    "    for value_to_replace, replacement_value in value_changes_map.items():\n",
    "        if value_to_replace in data[cityColumn].values:\n",
    "            has_changed = True\n",
    "            data_cleaned[cityColumn] = data_cleaned[cityColumn].replace(value_to_replace, replacement_value)\n",
    "            print(f\"Replaced '{value_to_replace}' with '{replacement_value}' in '{file}'.\")\n",
    "    \n",
    "    if has_changed:\n",
    "        # Update the CSV with cleaned data\n",
    "        data_cleaned.to_csv(filePath, index=False)\n",
    "    \n",
    "    # Add city-price pairs to the dictionary\n",
    "    for _, row in data_cleaned.iterrows():\n",
    "        city = row[cityColumn]\n",
    "        price = row[priceColumn]\n",
    "        if city in city_prices_map:\n",
    "            city_prices_map[city].append(price)\n",
    "        else:\n",
    "            city_prices_map[city] = [price]      \n",
    "\n",
    "# get the average of prices for each city\n",
    "city_to_avg_prices = {city: np.average(city_prices_map[city]) for city in city_prices_map.keys()}\n",
    "print(city_to_avg_prices)\n",
    "\n",
    "# Create a consistent mapping\n",
    "sorted_cities = sorted(city_to_avg_prices.items(), key=lambda x: x[1], reverse=True)  # Sort by valuation descending\n",
    "city_to_num = {city: idx for idx, (city, _) in enumerate(sorted_cities, start=1)}  # Start encoding from 1\n",
    "\n",
    "print(city_to_num)\n",
    "\n",
    "## Save the mapping in a json file\n",
    "JsonFileName = \"city_mapping.json\"\n",
    "JsonFilePath = getFilePath(JsonFileName, isSaveMapContext=True)\n",
    "if not os.path.exists(JsonFilePath):\n",
    "    with open(JsonFilePath, \"w\") as json_file:\n",
    "        json.dump(city_to_num, json_file, indent=4)\n",
    "    print(f\"JSON file '{JsonFilePath}' created.\")\n",
    "else:\n",
    "    print(f\"JSON file '{JsonFilePath}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Encode the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, cityColumn in FILES_TO_CITIES_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "\n",
    "    data = pd.read_csv(filePath)\n",
    "    \n",
    "    # Replace city names with numbers\n",
    "    data[cityColumn] = data[cityColumn].map(city_to_num)\n",
    "    \n",
    "    # Save the processed file\n",
    "    data.to_csv(filePath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Regions Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a- Create the regions encoding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'tayara.csv' has 1 rows removed due to missing 'city' values.\n",
      "{'ariana', 'ben arous', 'manouba', 'tunis'}\n",
      "{'ariana': 0, 'ben arous': 1, 'manouba': 2, 'tunis': 3}\n",
      "JSON file 'C:\\Users\\mohamedanas.neji\\OneDrive - Medius\\Desktop\\Housing_pricing\\utils\\region_mapping.json' created.\n"
     ]
    }
   ],
   "source": [
    "all_regions = set()\n",
    "for file, regionColumn in FILES_TO_REGION_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "\n",
    "    data = pd.read_csv(filePath)\n",
    "\n",
    "    nan_count = data[regionColumn].isna().sum()\n",
    "    data_cleaned = data\n",
    "    has_changed = False\n",
    "    if(nan_count > 0):\n",
    "        has_changed = True\n",
    "        # Remove rows with NaN in the city column\n",
    "        data_cleaned = data.dropna(subset=[regionColumn])\n",
    "        print(f\"File '{file}' has {len(data) - len(data_cleaned)} rows removed due to missing '{regionColumn}' values.\")\n",
    "\n",
    "    if has_changed:\n",
    "        # Update the CSV with cleaned data\n",
    "        data_cleaned.to_csv(filePath, index=False)\n",
    "    \n",
    "    # Add unique regions to the set\n",
    "    all_regions.update(data_cleaned[regionColumn].unique())\n",
    "\n",
    "print(all_regions)\n",
    "\n",
    "# Create a consistent mapping\n",
    "all_regions = sorted(all_regions)  # Sort for consistency\n",
    "region_to_num = {region: idx for idx, region in enumerate(all_regions)}\n",
    "\n",
    "print(region_to_num)\n",
    "\n",
    "## Save the mapping in a json file\n",
    "JsonFileName = \"region_mapping.json\"\n",
    "JsonFilePath = getFilePath(JsonFileName, isSaveMapContext=True)\n",
    "if not os.path.exists(JsonFilePath):\n",
    "    with open(JsonFilePath, \"w\") as json_file:\n",
    "        json.dump(city_to_num, json_file, indent=4)\n",
    "    print(f\"JSON file '{JsonFilePath}' created.\")\n",
    "else:\n",
    "    print(f\"JSON file '{JsonFilePath}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Encode the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, regionColumn in FILES_TO_REGION_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "\n",
    "    data = pd.read_csv(filePath)\n",
    "    \n",
    "    # Replace city names with numbers\n",
    "    data[regionColumn] = data[regionColumn].map(region_to_num)\n",
    "    \n",
    "    # Save the processed file\n",
    "    data.to_csv(filePath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
