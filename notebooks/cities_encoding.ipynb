{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cities/Regions Encoding**\n",
    "\n",
    "### **What to Expect ?**\n",
    "\n",
    "This notebook aims to do the following \n",
    " - Encode the cities\n",
    " - Encode the regions\n",
    "\n",
    "This will be done by creating 2 maps\n",
    " - one for cities encoding\n",
    " - one for regions encoding\n",
    "\n",
    "And replacing the string values by their encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## necessary imports here\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions/consts\n",
    "\n",
    "def getFilePath(fileName, isSaveMapContext):\n",
    "    templateToUse = \"C:\\\\Users\\\\mohamedanas.neji\\\\OneDrive - Medius\\\\Desktop\\\\Housing_pricing\\\\utils\\\\\" if isSaveMapContext else \"C:\\\\Users\\\\mohamedanas.neji\\\\OneDrive - Medius\\\\Desktop\\\\Housing_pricing\\\\data\\\\raw\\\\\" \n",
    "    return f\"{templateToUse}{fileName}\"\n",
    "\n",
    "## dataset_clean.csv has location and city but city is more precise so we will be removing location in a seperate notebook\n",
    "## tunisia-real-estate.csv has duplicated columns (Delegation and Locality) so we will be removing locality in another notebook\n",
    "## tunisie_annonce_data(1).csv doesnt have a column for cities so we won't do them\n",
    "FILES_TO_CITIES_COLUMN = {\n",
    "    \"dataset_clean.csv\": \"city\", \n",
    "    \"Property Prices in Tunisia.csv\": \"region\",\n",
    "    \"tayara.csv\": \"region\",\n",
    "    \"TechnocasaDataset.csv\": \"Subtitle\",\n",
    "    \"tunisia-real-estate.csv\": \"Delegation\"\n",
    "}\n",
    "\n",
    "## TechnocasaDataset.csv doesn't have a column for region so we won't be doing them\n",
    "FILES_TO_REGION_COLUMN = {\n",
    "    \"dataset_clean.csv\": \"governorate\", \n",
    "    \"Property Prices in Tunisia.csv\": \"city\",\n",
    "    \"tayara.csv\": \"city\",\n",
    "    \"tunisia-real-estate.csv\": \"Governorate\",\n",
    "    \"tunisie_annonce_data (1).csv\": \"Gouvernorat\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. cities encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a- Create the cities encoding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 'mohamadia' with 'mohamedia' in 'dataset_clean.csv'.\n",
      "Replaced 'mégrine' with 'megrine' in 'dataset_clean.csv'.\n",
      "Replaced 'el omrane superieur' with 'omrane superieur' in 'dataset_clean.csv'.\n",
      "Replaced 'mohamadia' with 'mohamedia' in 'tunisia-real-estate.csv'.\n",
      "Replaced 'mannouba' with 'manouba' in 'tunisia-real-estate.csv'.\n",
      "Replaced 'kabbaria' with 'kabaria' in 'tunisia-real-estate.csv'.\n",
      "Replaced 'ouerdia' with 'el ouardia' in 'tunisia-real-estate.csv'.\n",
      "{'bab bhar', 'manouba ville', 'ben arous', 'cite el khadra', 'denden', 'lac 2', 'oued ellil', 'sidi thabet', 'ksar said', 'manouba', 'jardins de carthage', 'hammam lif', 'medina jedida', 'kalaat landalous', 'rades', 'chotrana 3', 'la goulette', 'ennasr', 'borj louzir', 'ariana', 'menzah 9', 'menzah 7', 'tunis', 'sidi daoud', 'soukra', 'borj cedria', 'carthage', 'omrane superieur', 'megrine', 'manar 1', 'chotrana 2', 'raoued', 'menzah', 'mutuelleville', 'cite olympique', 'sidi bou said', 'ouardia', 'fouchana', 'menzah 6', 'ezzahra', 'mohamedia', 'jardins el menzah', 'bardo', 'gammarth', 'autres villes', 'manar 2', 'hammam chott', 'ain zaghouan', 'agba', 'hrairia', 'manar', 'riadh andalous', 'mourouj 4', 'tunis belvedere', 'mornag', 'lac 1', 'jebel jelloud', 'ghazela', 'mourouj', 'menzah 5', 'ennasr 2', 'cite ennkhilet', 'ezzouhour', 'dar fadhal', 'jardins menzah 1', 'chotrana', 'medina', 'kram', 'menzah 1', 'jedaida', 'ain zaghouan nord', 'chotrana 1', 'el ouardia', 'omrane', 'monfleury', 'boumhel', 'kabaria', 'aouina', 'boumhel el bassatine', 'ariana essoughra', 'mourouj 6', 'mourouj 5', 'tebourba', 'mnihla', 'douar hicher', 'jardins menzah 2', 'ariana ville', 'ettahrir', 'centre urbain nord', 'mourouj 1', 'marsa', 'ain zaghouan sud', 'sidi el bechir', 'sidi hassine', 'ettadhamen', 'mornaguia', 'lafayette', 'menzah 8', 'bab souika'}\n",
      "{'agba': 0, 'ain zaghouan': 1, 'ain zaghouan nord': 2, 'ain zaghouan sud': 3, 'aouina': 4, 'ariana': 5, 'ariana essoughra': 6, 'ariana ville': 7, 'autres villes': 8, 'bab bhar': 9, 'bab souika': 10, 'bardo': 11, 'ben arous': 12, 'borj cedria': 13, 'borj louzir': 14, 'boumhel': 15, 'boumhel el bassatine': 16, 'carthage': 17, 'centre urbain nord': 18, 'chotrana': 19, 'chotrana 1': 20, 'chotrana 2': 21, 'chotrana 3': 22, 'cite el khadra': 23, 'cite ennkhilet': 24, 'cite olympique': 25, 'dar fadhal': 26, 'denden': 27, 'douar hicher': 28, 'el ouardia': 29, 'ennasr': 30, 'ennasr 2': 31, 'ettadhamen': 32, 'ettahrir': 33, 'ezzahra': 34, 'ezzouhour': 35, 'fouchana': 36, 'gammarth': 37, 'ghazela': 38, 'hammam chott': 39, 'hammam lif': 40, 'hrairia': 41, 'jardins de carthage': 42, 'jardins el menzah': 43, 'jardins menzah 1': 44, 'jardins menzah 2': 45, 'jebel jelloud': 46, 'jedaida': 47, 'kabaria': 48, 'kalaat landalous': 49, 'kram': 50, 'ksar said': 51, 'la goulette': 52, 'lac 1': 53, 'lac 2': 54, 'lafayette': 55, 'manar': 56, 'manar 1': 57, 'manar 2': 58, 'manouba': 59, 'manouba ville': 60, 'marsa': 61, 'medina': 62, 'medina jedida': 63, 'megrine': 64, 'menzah': 65, 'menzah 1': 66, 'menzah 5': 67, 'menzah 6': 68, 'menzah 7': 69, 'menzah 8': 70, 'menzah 9': 71, 'mnihla': 72, 'mohamedia': 73, 'monfleury': 74, 'mornag': 75, 'mornaguia': 76, 'mourouj': 77, 'mourouj 1': 78, 'mourouj 4': 79, 'mourouj 5': 80, 'mourouj 6': 81, 'mutuelleville': 82, 'omrane': 83, 'omrane superieur': 84, 'ouardia': 85, 'oued ellil': 86, 'rades': 87, 'raoued': 88, 'riadh andalous': 89, 'sidi bou said': 90, 'sidi daoud': 91, 'sidi el bechir': 92, 'sidi hassine': 93, 'sidi thabet': 94, 'soukra': 95, 'tebourba': 96, 'tunis': 97, 'tunis belvedere': 98}\n",
      "JSON file 'C:\\Users\\mohamedanas.neji\\OneDrive - Medius\\Desktop\\Housing_pricing\\utils\\city_mapping.json' created.\n"
     ]
    }
   ],
   "source": [
    "all_cities = set()\n",
    "value_changes_map = {\"mohamadia\": \"mohamedia\", \"mannouba\": \"manouba\", \"mégrine\": \"megrine\", \"kabbaria\": \"kabaria\", \"ouerdia\": \"el ouardia\", \"el omrane superieur\": \"omrane superieur\"}\n",
    "for file, cityColumn in FILES_TO_CITIES_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "    data = pd.read_csv(filePath)\n",
    "    has_changed = False\n",
    "    nan_count = data[cityColumn].isna().sum()\n",
    "    data_cleaned = data\n",
    "    if(nan_count > 0):\n",
    "        has_changed = True\n",
    "        # Remove rows with NaN in the city column\n",
    "        data_cleaned = data.dropna(subset=[cityColumn])\n",
    "        print(f\"File '{file}' has {len(data) - len(data_cleaned)} rows removed due to missing '{cityColumn}' values.\")\n",
    "    \n",
    "    # Some cities needs to be changed to ave the same name (sahitek sidahmed hhhhhh)\n",
    "    for value_to_replace, replacement_value in value_changes_map.items():\n",
    "        if value_to_replace in data[cityColumn].values:\n",
    "            has_changed = True\n",
    "            data_cleaned[cityColumn] = data_cleaned[cityColumn].replace(value_to_replace, replacement_value)\n",
    "            print(f\"Replaced '{value_to_replace}' with '{replacement_value}' in '{file}'.\")\n",
    "    \n",
    "    if has_changed:\n",
    "        # Update the CSV with cleaned data\n",
    "        data_cleaned.to_csv(filePath, index=False)\n",
    "    \n",
    "    # Add unique cities to the set\n",
    "    all_cities.update(data_cleaned[cityColumn].unique())\n",
    "\n",
    "print(all_cities)\n",
    "\n",
    "# Create a consistent mapping\n",
    "all_cities = sorted(all_cities)  # Sort for consistency\n",
    "city_to_num = {city: idx for idx, city in enumerate(all_cities)}\n",
    "\n",
    "print(city_to_num)\n",
    "\n",
    "## Save the mapping in a json file\n",
    "JsonFileName = \"city_mapping.json\"\n",
    "JsonFilePath = getFilePath(JsonFileName, isSaveMapContext=True)\n",
    "if not os.path.exists(JsonFilePath):\n",
    "    with open(JsonFilePath, \"w\") as json_file:\n",
    "        json.dump(city_to_num, json_file, indent=4)\n",
    "    print(f\"JSON file '{JsonFilePath}' created.\")\n",
    "else:\n",
    "    print(f\"JSON file '{JsonFilePath}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Encode the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, cityColumn in FILES_TO_CITIES_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "\n",
    "    data = pd.read_csv(filePath)\n",
    "    \n",
    "    # Replace city names with numbers\n",
    "    data[cityColumn] = data[cityColumn].map(city_to_num)\n",
    "    \n",
    "    # Save the processed file\n",
    "    data.to_csv(filePath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Regions Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a- Create the regions encoding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'tayara.csv' has 1 rows removed due to missing 'city' values.\n",
      "{'ariana', 'ben arous', 'manouba', 'tunis'}\n",
      "{'ariana': 0, 'ben arous': 1, 'manouba': 2, 'tunis': 3}\n",
      "JSON file 'C:\\Users\\mohamedanas.neji\\OneDrive - Medius\\Desktop\\Housing_pricing\\utils\\region_mapping.json' created.\n"
     ]
    }
   ],
   "source": [
    "all_regions = set()\n",
    "for file, regionColumn in FILES_TO_REGION_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "\n",
    "    data = pd.read_csv(filePath)\n",
    "\n",
    "    nan_count = data[regionColumn].isna().sum()\n",
    "    data_cleaned = data\n",
    "    has_changed = False\n",
    "    if(nan_count > 0):\n",
    "        has_changed = True\n",
    "        # Remove rows with NaN in the city column\n",
    "        data_cleaned = data.dropna(subset=[regionColumn])\n",
    "        print(f\"File '{file}' has {len(data) - len(data_cleaned)} rows removed due to missing '{regionColumn}' values.\")\n",
    "\n",
    "    if has_changed:\n",
    "        # Update the CSV with cleaned data\n",
    "        data_cleaned.to_csv(filePath, index=False)\n",
    "    \n",
    "    # Add unique regions to the set\n",
    "    all_regions.update(data_cleaned[regionColumn].unique())\n",
    "\n",
    "print(all_regions)\n",
    "\n",
    "# Create a consistent mapping\n",
    "all_regions = sorted(all_regions)  # Sort for consistency\n",
    "region_to_num = {region: idx for idx, region in enumerate(all_regions)}\n",
    "\n",
    "print(region_to_num)\n",
    "\n",
    "## Save the mapping in a json file\n",
    "JsonFileName = \"region_mapping.json\"\n",
    "JsonFilePath = getFilePath(JsonFileName, isSaveMapContext=True)\n",
    "if not os.path.exists(JsonFilePath):\n",
    "    with open(JsonFilePath, \"w\") as json_file:\n",
    "        json.dump(city_to_num, json_file, indent=4)\n",
    "    print(f\"JSON file '{JsonFilePath}' created.\")\n",
    "else:\n",
    "    print(f\"JSON file '{JsonFilePath}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Encode the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, regionColumn in FILES_TO_REGION_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "\n",
    "    data = pd.read_csv(filePath)\n",
    "    \n",
    "    # Replace city names with numbers\n",
    "    data[regionColumn] = data[regionColumn].map(region_to_num)\n",
    "    \n",
    "    # Save the processed file\n",
    "    data.to_csv(filePath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
