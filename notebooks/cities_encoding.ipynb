{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cities/Regions Encoding**\n",
    "\n",
    "### **What to Expect ?**\n",
    "\n",
    "This notebook aims to do the following \n",
    " - Encode the cities\n",
    " - Encode the regions\n",
    "\n",
    "This will be done by creating 2 maps\n",
    " - one for cities encoding\n",
    " - one for regions encoding\n",
    "\n",
    "And replacing the string values by their encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## necessary imports here\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions/consts\n",
    "\n",
    "def getFilePath(fileName, isSaveMapContext):\n",
    "    templateToUse = \"C:\\\\Users\\\\mohamedanas.neji\\\\OneDrive - Medius\\\\Desktop\\\\Housing_pricing\\\\utils\\\\\" if isSaveMapContext else \"C:\\\\Users\\\\mohamedanas.neji\\\\OneDrive - Medius\\\\Desktop\\\\Housing_pricing\\\\data\\\\raw\\\\\" \n",
    "    return f\"{templateToUse}{fileName}\"\n",
    "\n",
    "## dataset_clean.csv has location and city but city is more precise so we will be removing location in a seperate notebook\n",
    "## tunisia-real-estate.csv has duplicated columns (Delegation and Locality) so we will be removing locality in another notebook\n",
    "## tunisie_annonce_data(1).csv doesnt have a column for cities so we won't do them\n",
    "FILES_TO_CITIES_COLUMN = {\n",
    "    \"dataset_clean.csv\": \"city\", \n",
    "    \"Property Prices in Tunisia_Cleaned.csv\": \"region\",\n",
    "    \"tayara.csv\": \"region\",\n",
    "    \"TechnocasaDataset.csv\": \"Subtitle\",\n",
    "    \"tunisia-real-estate.csv\": \"Delegation\"\n",
    "}\n",
    "\n",
    "## TechnocasaDataset.csv doesn't have a column for region so we won't be doing them\n",
    "FILES_TO_REGION_COLUMN = {\n",
    "    \"dataset_clean.csv\": \"governorate\", \n",
    "    \"Property Prices in Tunisia_Cleaned.csv\": \"city\",\n",
    "    \"tayara.csv\": \"city\",\n",
    "    \"tunisia-real-estate.csv\": \"Governorate\",\n",
    "    \"tunisie_annonce_data (1).csv\": \"Gouvernorat\"\n",
    "}\n",
    "\n",
    "FILES_TO_PRICE_COLUMN = {\n",
    "    \"dataset_clean.csv\": \"price_tnd\", \n",
    "    \"Property Prices in Tunisia_Cleaned.csv\": \"price\",\n",
    "    \"tayara.csv\": \"price\",\n",
    "    \"tunisia-real-estate.csv\": \"Price\",\n",
    "    \"tunisie_annonce_data (1).csv\": \"Price\",\n",
    "    \"TechnocasaDataset.csv\": \"Price\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. cities encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a- Create the cities encoding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 'mohamadia' with 'mohamedia' in 'dataset_clean.csv'.\n",
      "Replaced 'mégrine' with 'megrine' in 'dataset_clean.csv'.\n",
      "Replaced 'el omrane superieur' with 'omrane superieur' in 'dataset_clean.csv'.\n",
      "Replaced 'ouardia' with 'el ouardia' in 'Property Prices in Tunisia_Cleaned.csv'.\n",
      "Replaced 'mannouba' with 'manouba' in 'tunisia-real-estate.csv'.\n",
      "Replaced 'kabbaria' with 'kabaria' in 'tunisia-real-estate.csv'.\n",
      "Replaced 'ouerdia' with 'el ouardia' in 'tunisia-real-estate.csv'.\n",
      "{'menzah': 460251.18483412324, 'marsa': 517180.875739645, 'bardo': 349817.0731707317, 'mornaguia': 245000.0, 'soukra': 453755.64416058396, 'kram': 601161.125, 'mnihla': 319704.54545454547, 'ariana ville': 347088.90995260666, 'omrane': 349384.6153846154, 'tunis': 334365.04761904763, 'ezzahra': 292863.1263157895, 'manouba': 217466.43835616438, 'megrine': 395213.9534883721, 'mornag': 208782.60869565216, 'el ouardia': 285014.81481481483, 'omrane superieur': 268500.0, 'boumhel el bassatine': 328196.77419354836, 'ettadhamen': 216250.0, 'la goulette': 474872.3404255319, 'hammam lif': 219421.05263157896, 'raoued': 277459.0, 'rades': 220670.09278350516, 'carthage': 540176.4705882353, 'oued ellil': 207744.73333333334, 'medina jedida': 216476.05042016806, 'kabaria': 122600.0, 'fouchana': 197000.0, 'hammam chott': 198609.0, 'mourouj': 187564.67289719626, 'cite el khadra': 315209.3023255814, 'hrairia': 135333.33333333334, 'kalaat landalous': 300000.0, 'ben arous': 191109.0909090909, 'sidi hassine': 126690.90909090909, 'ettahrir': 184133.33333333334, 'jedaida': 112500.0, 'mohamedia': 193599.6, 'jardins el menzah': 249522.0588235294, 'ennasr': 335731.1538461539, 'borj louzir': 164600.0, 'ghazela': 335597.1074380165, 'ariana': 194615.38461538462, 'chotrana': 180214.2857142857, 'sidi thabet': 207500.0, 'boumhel': 256799.0, 'manouba ville': 196615.38461538462, 'denden': 248642.85714285713, 'douar hicher': 102222.22222222222, 'aouina': 289275.33640552993, 'agba': 133285.7142857143, 'sidi daoud': 454000.0, 'manar': 349166.6666666667, 'ezzouhour': 85000.0, 'centre urbain nord': 247000.0, 'medina': 66000.0, 'lafayette': 180000.0, 'sidi bou said': 260000.0, 'mutuelleville': 250000.0, 'chotrana 1': 406000.0, 'gammarth': 563500.0, 'ennasr 2': 336737.5, 'jardins menzah 1': 256250.0, 'borj cedria': 448960.0, 'lac 1': 717500.0, 'jardins de carthage': 636251.4807692308, 'chotrana 2': 532593.8571428572, 'menzah 6': 282125.0, 'monfleury': 180000.0, 'ain zaghouan': 451151.55629139073, 'lac 2': 1284400.0, 'riadh andalous': 333111.1111111111, 'ain zaghouan nord': 385384.6153846154, 'menzah 7': 317133.3333333333, 'manar 1': 258331.0, 'cite olympique': 233750.0, 'ariana essoughra': 379800.0, 'manar 2': 613750.0, 'ain zaghouan sud': 549250.0, 'menzah 9': 980571.4285714285, 'chotrana 3': 1069999.0, 'mourouj 6': 203500.0, 'jardins menzah 2': 344500.0, 'menzah 5': 422546.3157894737, 'mourouj 1': 134750.0, 'mourouj 5': 130571.42857142857, 'mourouj 4': 200000.0, 'tunis belvedere': 300000.0, 'menzah 1': 190000.0, 'bab souika': 234692.3076923077, 'cite ennkhilet': 143000.0, 'ksar said': 117333.33333333333, 'dar fadhal': 386000.0, 'sidi el bechir': 168444.44444444444, 'bab bhar': 217544.55445544556, 'tebourba': 180000.0, 'jebel jelloud': 140000.0}\n",
      "{'lac 2': 1, 'chotrana 3': 2, 'menzah 9': 3, 'lac 1': 4, 'jardins de carthage': 5, 'manar 2': 6, 'kram': 7, 'gammarth': 8, 'ain zaghouan sud': 9, 'carthage': 10, 'chotrana 2': 11, 'marsa': 12, 'la goulette': 13, 'menzah': 14, 'sidi daoud': 15, 'soukra': 16, 'ain zaghouan': 17, 'borj cedria': 18, 'menzah 5': 19, 'chotrana 1': 20, 'megrine': 21, 'dar fadhal': 22, 'ain zaghouan nord': 23, 'ariana essoughra': 24, 'bardo': 25, 'omrane': 26, 'manar': 27, 'ariana ville': 28, 'jardins menzah 2': 29, 'ennasr 2': 30, 'ennasr': 31, 'ghazela': 32, 'tunis': 33, 'riadh andalous': 34, 'boumhel el bassatine': 35, 'mnihla': 36, 'menzah 7': 37, 'cite el khadra': 38, 'kalaat landalous': 39, 'tunis belvedere': 40, 'ezzahra': 41, 'aouina': 42, 'el ouardia': 43, 'menzah 6': 44, 'raoued': 45, 'omrane superieur': 46, 'sidi bou said': 47, 'manar 1': 48, 'boumhel': 49, 'jardins menzah 1': 50, 'mutuelleville': 51, 'jardins el menzah': 52, 'denden': 53, 'centre urbain nord': 54, 'mornaguia': 55, 'bab souika': 56, 'cite olympique': 57, 'rades': 58, 'hammam lif': 59, 'bab bhar': 60, 'manouba': 61, 'medina jedida': 62, 'ettadhamen': 63, 'mornag': 64, 'oued ellil': 65, 'sidi thabet': 66, 'mourouj 6': 67, 'mourouj 4': 68, 'hammam chott': 69, 'fouchana': 70, 'manouba ville': 71, 'ariana': 72, 'mohamedia': 73, 'ben arous': 74, 'menzah 1': 75, 'mourouj': 76, 'ettahrir': 77, 'chotrana': 78, 'lafayette': 79, 'monfleury': 80, 'tebourba': 81, 'sidi el bechir': 82, 'borj louzir': 83, 'cite ennkhilet': 84, 'jebel jelloud': 85, 'hrairia': 86, 'mourouj 1': 87, 'agba': 88, 'mourouj 5': 89, 'sidi hassine': 90, 'kabaria': 91, 'ksar said': 92, 'jedaida': 93, 'douar hicher': 94, 'ezzouhour': 95, 'medina': 96}\n",
      "JSON file 'C:\\Users\\mohamedanas.neji\\OneDrive - Medius\\Desktop\\Housing_pricing\\utils\\city_mapping.json' created.\n"
     ]
    }
   ],
   "source": [
    "city_prices_map = {}\n",
    "value_changes_map = {\"mohamadia\": \"mohamedia\", \"mannouba\": \"manouba\", \"mégrine\": \"megrine\", \"kabbaria\": \"kabaria\", \"ouardia\": \"el ouardia\",\"ouerdia\": \"el ouardia\", \"el omrane superieur\": \"omrane superieur\"}\n",
    "for file, cityColumn in FILES_TO_CITIES_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "    priceColumn = FILES_TO_PRICE_COLUMN[file]\n",
    "    data = pd.read_csv(filePath)\n",
    "    has_changed = False\n",
    "    nan_count = data[cityColumn].isna().sum()\n",
    "    data_cleaned = data\n",
    "    if(nan_count > 0):\n",
    "        has_changed = True\n",
    "        # Remove rows with NaN in the city column\n",
    "        data_cleaned = data.dropna(subset=[cityColumn])\n",
    "        print(f\"File '{file}' has {len(data) - len(data_cleaned)} rows removed due to missing '{cityColumn}' values.\")\n",
    "    \n",
    "    # Some cities needs to be changed to ave the same name (sahitek sidahmed hhhhhh)\n",
    "    for value_to_replace, replacement_value in value_changes_map.items():\n",
    "        if value_to_replace in data[cityColumn].values:\n",
    "            has_changed = True\n",
    "            data_cleaned[cityColumn] = data_cleaned[cityColumn].replace(value_to_replace, replacement_value)\n",
    "            print(f\"Replaced '{value_to_replace}' with '{replacement_value}' in '{file}'.\")\n",
    "    \n",
    "    if has_changed:\n",
    "        # Update the CSV with cleaned data\n",
    "        data_cleaned.to_csv(filePath, index=False)\n",
    "    \n",
    "    # Add city-price pairs to the dictionary\n",
    "    for _, row in data_cleaned.iterrows():\n",
    "        city = row[cityColumn]\n",
    "        price = row[priceColumn]\n",
    "        if city in city_prices_map:\n",
    "            city_prices_map[city].append(price)\n",
    "        else:\n",
    "            city_prices_map[city] = [price]      \n",
    "\n",
    "# get the average of prices for each city\n",
    "city_to_avg_prices = {city: np.average(city_prices_map[city]) for city in city_prices_map.keys()}\n",
    "print(city_to_avg_prices)\n",
    "\n",
    "# Create a consistent mapping\n",
    "sorted_cities = sorted(city_to_avg_prices.items(), key=lambda x: x[1], reverse=True)  # Sort by valuation descending\n",
    "city_to_num = {city: idx for idx, (city, _) in enumerate(sorted_cities, start=1)}  # Start encoding from 1\n",
    "\n",
    "print(city_to_num)\n",
    "\n",
    "## Save the mapping in a json file\n",
    "JsonFileName = \"city_mapping.json\"\n",
    "JsonFilePath = getFilePath(JsonFileName, isSaveMapContext=True)\n",
    "if not os.path.exists(JsonFilePath):\n",
    "    with open(JsonFilePath, \"w\") as json_file:\n",
    "        json.dump(city_to_num, json_file, indent=4)\n",
    "    print(f\"JSON file '{JsonFilePath}' created.\")\n",
    "else:\n",
    "    print(f\"JSON file '{JsonFilePath}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Encode the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, cityColumn in FILES_TO_CITIES_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "\n",
    "    data = pd.read_csv(filePath)\n",
    "    \n",
    "    # Replace city names with numbers\n",
    "    data[cityColumn] = data[cityColumn].map(city_to_num)\n",
    "    \n",
    "    # Save the processed file\n",
    "    data.to_csv(filePath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Regions Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a- Create the regions encoding map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'tayara.csv' has 1 rows removed due to missing 'city' values.\n",
      "{'ariana', 'ben arous', 'manouba', 'tunis'}\n",
      "{'ariana': 0, 'ben arous': 1, 'manouba': 2, 'tunis': 3}\n",
      "JSON file 'C:\\Users\\mohamedanas.neji\\OneDrive - Medius\\Desktop\\Housing_pricing\\utils\\region_mapping.json' created.\n"
     ]
    }
   ],
   "source": [
    "all_regions = set()\n",
    "for file, regionColumn in FILES_TO_REGION_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "    priceColumn = FILES_TO_PRICE_COLUMN[file]\n",
    "    data = pd.read_csv(filePath)\n",
    "\n",
    "    nan_count = data[regionColumn].isna().sum()\n",
    "    data_cleaned = data\n",
    "    has_changed = False\n",
    "    if(nan_count > 0):\n",
    "        has_changed = True\n",
    "        # Remove rows with NaN in the city column\n",
    "        data_cleaned = data.dropna(subset=[regionColumn])\n",
    "        print(f\"File '{file}' has {len(data) - len(data_cleaned)} rows removed due to missing '{regionColumn}' values.\")\n",
    "\n",
    "    if has_changed:\n",
    "        # Update the CSV with cleaned data\n",
    "        data_cleaned.to_csv(filePath, index=False)\n",
    "    \n",
    "    # Add unique regions to the set\n",
    "    all_regions.update(data_cleaned[regionColumn].unique())\n",
    "\n",
    "print(all_regions)\n",
    "\n",
    "# Create a consistent mapping\n",
    "all_regions = sorted(all_regions)  # Sort for consistency\n",
    "region_to_num = {region: idx for idx, region in enumerate(all_regions)}\n",
    "\n",
    "print(region_to_num)\n",
    "\n",
    "## Save the mapping in a json file\n",
    "JsonFileName = \"region_mapping.json\"\n",
    "JsonFilePath = getFilePath(JsonFileName, isSaveMapContext=True)\n",
    "if not os.path.exists(JsonFilePath):\n",
    "    with open(JsonFilePath, \"w\") as json_file:\n",
    "        json.dump(city_to_num, json_file, indent=4)\n",
    "    print(f\"JSON file '{JsonFilePath}' created.\")\n",
    "else:\n",
    "    print(f\"JSON file '{JsonFilePath}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Encode the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, regionColumn in FILES_TO_REGION_COLUMN.items():\n",
    "    filePath = getFilePath(file, isSaveMapContext=False)\n",
    "\n",
    "    data = pd.read_csv(filePath)\n",
    "    \n",
    "    # Replace city names with numbers\n",
    "    data[regionColumn] = data[regionColumn].map(region_to_num)\n",
    "    \n",
    "    # Save the processed file\n",
    "    data.to_csv(filePath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
